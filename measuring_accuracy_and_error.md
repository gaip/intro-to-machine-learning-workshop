Measuring the accuracy of our models

True positive = check engine light is on and there's a problem
True negative = check engine light is off and everything is OK
False positive = TYPE I ERROR = check engine light is on and everything is OK
False negative = TYPE II ERROR = check engine light is off BUT we've got a missing drain bolt

Accuracy
    Overall, how often is the classifier correct?
    (Number of True Positives + True Negatives) / total

Precision
    When it predicts yes, how often is it correct?
    True Positives over / predicted yes = 100/110 = 0.91
    True Positives / (True Positives + False Positives)

True Positive Rate AKA Recall or Sensitivity
When it's actually yes, how often does it predict yes?
TP/actual yes = 100/105 = 0.95


F Score: This is a weighted average of the true positive rate (recall) and precision. 

ROC Curve: This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class. 